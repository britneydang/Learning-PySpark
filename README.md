# Learning-PySpark 
This project contain many mini projects written in PySPark with other dependencies. Instead of writing in Databricks, I wanted to experiment the hardship of not to have a new technology (Databricks). 
I believed my two dependencies' version (JAVA JDK or Hadoop winutils) were not incompatible to Pyspark version. It led to an impossibility to export an output to a designated folder as well as read all files inside a designated folder. There were some errors in the jupyter lab due to the incompatible versions.

Set up:
- JAVA JDK SE 8
- Python 3.10.0
- Pyspark 3.2.1
- Hadoop (winutils 3.2.1)
- Environment variables (JAVA_HOME, HADOOP_HOME, paths...)
- (use cmd)
